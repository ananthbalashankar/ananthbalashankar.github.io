<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"

        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

	<title>Ananth Balashankar's Webpage</title>
	
	<link rel="stylesheet" href="grad.css" type="text/css" media="all" />

</head>

<body>
<!--div id="wrapper" -->
<br />
<div id="masthead">
	
	<div id="bannerphoto">
		<img src="profile.jpg" style="width:200px">
	</div>

	<div id="bannertext">
		<a href="." style="font-size:200%;color:#ffffff">Ananth Balashankar</a><br />
		Senior Research Scientist, Google DeepMind<br />		
		New York<br />
		ananth(at)nyu(dot)edu
		<br>
		<b><a href="CV_Latest.pdf">[CV]<a/>  &emsp; <a href="Ananth-Research.pdf">[Research Statement]</a> &emsp; <a href="https://www.linkedin.com/in/ananth-balashankar-02093852/">[LinkedIn]</a> &emsp; </b><br>
	</div>

</div>  <!--masthead-->

<div>
	<p>I'm a Senior Research Scientist at Google DeepMind in the GenAI Safety and Alignment team in New York. I have contributed to the launches of Gemini 2.0, 2.5, AI Overviews and AI Mode. 
	Specifically, my research interests lie in adversarial training, reward model alignment and robustness. 
	You can find some of our recent papers <a href="https://scholar.google.com/citations?hl=en&user=dr5VLwEAAAAJ&view_op=list_works&sortby=pubdate">here</a>.

	<p> Previously, in Google Research, I worked on improving the robustness and safety of classifiers and foundation models (LLMs) that power applications such as 
	YouTube ads monetization, Bard, and Search Generative Experience. I led work from basic research to product impact, with >10 launches that reach 1B+ users.
		
	<p>I got my Ph.D in Computer Science advised by <a href="https://cs.nyu.edu/~lakshmi/Lakshmi/Home.html"> Prof. Lakshminarayanan Subramanian </a> at NYU's Courant Institute of Mathematical Sciences 
	and <a href="http://alexbeutel.com/">Dr. Alex Beutel</a> (now at OpenAI). I was a Student Researcher at Google AI (2019-22), where I worked on counterfactual text robustness.
	I was a Software Engineer at Google for 3 years, where I worked on recommendations at Google Play Store and the Play Developer Console in Mountain View and London. 
	I graduated from the Indian Institute of Technology, Kharagpur with B.Tech/M.Tech in Computer Science advised by <a href="http://www.facweb.iitkgp.ac.in/~niloy/">Prof. Niloy Ganguly</a>.</p>

	
	<h2>Honors</h2>
	
	<ol> 
		<li>NYU Janet Fabri Prize for best Ph.D dissertation in Computer Science - 2023</li>
		<li>Google Student Research Advisor Program Fellowship (2019-2022)</li>
		<li>Spot bonus for research contributions in the Google Responsible AI team - 2021</li>
		<li>NYU Harold Grad Memorial Prize for promising Ph.D achievement - 2019</li>
		<li>Best Paper Award at NAACL 2024 TrustNLP, and ICML 2019 AI for social good workshops</li>
		<li>MacCracken Fellowship (2017-22)</li>
	</ol>
	
	<h2>Research Interests</h2>
	I am interested in building safe and responsible ML models through methodologies including domain faithful optimization, data augmentation and causal feature selection.
	Broadly, my work has had demonstrable <i>business and research impact across five real world application domains</i>:
	<ol>
		
		<li><b>Safety and Responsibility in AI</b><br>
		Automated detection of online toxic comments improves the quality of interaction in social media.
		However, the variations in context of comments make it hard to protect specific demographic groups from disparate impact. 
		By explicitly modeling such nuances through counterfactual data augmentation, we improved the accuracy of detecting toxicity by 6%
		Through this <a href="#robust1">publication</a> at EMNLP '21, a premiere NLP conference, I have fostered deep engagements with Google's Responsible ML team.
		I have also <a href="#play">deployed</a> ML models that optimize business objectives like diversity at Google Play.</li><br>
		<li><b>ML Robustness</b><br>
		Robust ML models are critical in building high-stake applications and require a shift from traditional ML models that focus only on optimizing accuracy over the observed but limited test data.
		By incorporating rules and data from the real world, we have improved accuracy of state-of-the-art transformer based models by 12% in this <a href="#concordance">publication</a> at WSDM '21, the premiere data mining conference.</li><br>
		<li><b>Causal-Aware ML</b><br>
		Causality based question answering lies at the core of customer support tools like chatbots. 
		Prior ML models fail to capture the directed nature of causality, for example rain causes traffic delay, and not vice versa. 
		By learning asymmetric causal embeddings faithful to causal graphs, we improved accuracy on Yahoo! Answers by 21% in this <a href="#faithful">paper</a> at ACL '21, a premiere NLP conference.</li><br>
		<li><b>AI for Social Good</b><br>
		Forecasting famine is critical for the mobilization of aid to millions of people, but hard to solve due to data scarcity in fragile countries. 
		By building a news-based causal-aware forecasting framework that extracts causal features from 11.2 million news articles across 2 decades in 21 fragile countries, we have improved forecasting accuracy by 32% compared to state-of-the-art predictive models. 
		This <a href="#famine2">paper</a> is accepted at IC2S2 '21, the premiere computational social science conference, and at <a href="#famine">Science Advances</a> in 2023. 
		The tool will be used by the World Bank Data Science group for aid allocation on food security. 
		Based on this research, a few co-authors have founded a socio-economic inference start-up <a href="http://vel.ai/">Velai, Inc</a>.</li><br>
		<li><b>ML in Privacy</b><br>
		Corporate privacy compliance policies are legally prescriptive, but not directly enforceable in computer systems. 
		By using the theory of contextual integrity through post-processing mappings, we have improved the accuracy of BERT-based deep learning models by 6% to extract privacy parameters for SQL-based enforcement in this <a href="#privacy1">paper</a> at WWW' 19, the premiere web research conference.</li>
	</ol>
	
	<h2>Select Publications</h2>
	
	
	<h3>Adversarial Safety</h3>
		<ul>
		<li id="robust3"><b><u>Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks</u></b> <br>
		Aradhana Sinha*, Ananth Balashankar*, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel. TMLR 2023. <a href="https://arxiv.org/pdf/2310.16955.pdf">[pdf]</a>
		</li>
		<br>
		<li><b><u>Automated Adversarial Discovery for Safety Classifiers</u></b> <br>
		Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin, Ananth Balashankar. NAACL 2024 Workshop on Trustworthy NLP. <a href="https://arxiv.org/pdf/2406.17104">[pdf]</a>
		</li>
		<br>
		</ul>
	<h3>Reward Alignment</h3>
		<ul>
		<li><b><u>InfAlign: Inference-aware language model alignment</u></b> <br>
		Ananth Balashankar*, Ziteng Sun*, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, Ananda Theertha Suresh, Ahmad Beirami. ICML 2025. <a href="https://arxiv.org/pdf/2412.19792">[pdf]</a>
		</li>
		<br>
		<li><b><u>Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment</u></b> <br>
		Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, Ahmad Beirami. EMNLP 2024. <a href="https://arxiv.org/pdf/2404.12318">[pdf]</a>
		</li>
		<br>
		</ul>
	<h3>Robustness</h3>
	<ul>
		<li><b><u>Inducing group fairness in llm-based decisions</u></b> <br>
		James Atwood, Nino Scherrer, Preethi Lahoti, Ananth Balashankar, Flavien Prost, Ahmad Beirami. ICLR 2025 Workshop on Spurious Correlation and Shortcut Learning. <a href="https://arxiv.org/abs/2406.16738v1">[pdf]</a>
		</li>
		<br>
		<li><b><u>Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning</u></b> <br>
		Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel
		<i>Under Review.</i> <a href="https://arxiv.org/abs/2310.16959">[pdf]</a></li>
		<br>	
		<li id="robust2"><b><u>Improving Robustness through Pairwise Generative Counterfactual Data Augmentation</u></b> <br>
		Ananth Balashankar, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed Chi, Jilin Chen, and Alex Beutel. EMNLP 2023 (Findings). <a href="https://aclanthology.org/2023.findings-emnlp.10.pdf">[pdf]</a>
		</li>
		<br>
		<li id="robust1"><b><u>Can We Improve Model Robustness through Secondary Attribute Counterfactuals?</u></b> <br>
		Ananth Balashankar, Xuezhi Wang, Ben Packer, Nithum Thain, Ed Chi and Alex Beutel<br>
		<i>Conference on Empirical Methods in Natural Language Processing (EMNLP) 2021 (Main Conference Acceptance Rate: 22.4%) </i><a href="Context_Aware_Counterfactual_Robustness (24).pdf">[pdf]</a></li>
		<br>
	</ul>
	<h3>AI for social good</h3>
	<ul>
		<li id="famine"><b><u>Predicting food crises using news streams</u></b> <br>
		Ananth Balashankar, Lakshmi Subramanian, Samuel Fraiberger<br>
		<i>Science Advances, 2023 </i> <a href="https://www.science.org/doi/10.1126/sciadv.abm3449">[pdf]</a></li>
		<br>
		<li id="pollution1"><b><u>Spatio-temporal modeling of urban air quality using low-cost monitors</u></b> <br>
		Shiva Iyer, Ananth Balashankar, William Aeberhard, Sameeksha Jain, Sujoy Bhattacharya, Guiditta Rusconi, Anant Sudarshan, Rohini Pande, Lakshmi Subramanian<br>
		<i>NPJ Climate and Atmospheric Science (2022) </i><a href="https://www.nature.com/articles/s41612-022-00293-z">[link]</a></li>
		<br>
		<li id="africa"><b><u>Targeted Policy Recommendations using Outcome-aware Clustering</u></b> <br>
		Joint work with World Bank collaborators - Samuel Fraiberger, Marelize GÃ¶rgens, Clara Ivanescu, Andrew Longosz, Shaffiq Somani, Tushar Malik, Theo Hawkins; 
		Lakshmi Subramanian, Eric Deregt (NYU) and David Wilson (Bill and Melinda Gates Foundation)<br>
		<i> ACM COMPASS 2022. <a href="compass_rev_clustering_2021.pdf">[pdf]</a>World Bank Technical Report</i> 2018. 
			<a href="https://thedocs.worldbank.org/en/doc/163671541431652090-0090022018/original/AgricltureTransformationinAfrica.pdf">[pdf] </a> </li>
		<br>
	</ul>
	<h3>AI for privacy</h3>
	<ul>
		<li id="privacy2"><b><u>Beyond The Text: Analysis of Privacy Statements through Syntactic and Semantic Role Labeling</u></b> <br>
		Yan Shvartzshnaider, Ananth Balashankar, Thomas Wies, Lakshminarayanan Subramanian. 5th Natural Legal Language Processing (NLLP) Workshop at EMNLP 2023.
		<a href="https://arxiv.org/abs/2010.00678">[arxiv]</a></li>
		<br>
		<li id="privacy1"><b><u>VACCINE: Using Contextual Integrity for Data Leakage Detection</u> </b><br>
		Yan Shvartzshnaider, Zvonimir Pavlinovic, Ananth Balashankar, Thomas Wies, Lakshminarayanan Subramanian, Helen Nissenbaum and Prateek Mittal<br>
		<i> The Web Conference (WWW)</i> 2019 (Acceptance Rate: 18%). <a href="https://cs.nyu.edu/wies/publ/vaccine.pdf">[pdf]</a> </li>
		<br>
	</ul>
	<h2> Other Research </h2>
	<ul>
	<li id="pcg2"><b><u>Learning Conditional Granger Causal Temporal Networks</u></b> <br>
	Ananth Balashankar, Srikanth Jagabathula, Lakshminarayanan Subramanian.<br> Causal Learning and Reasoning Conference (CLeaR) 2023.
	<a href="granger_causal.pdf">[pdf]</a></li>
	<br>
	<li><b><u>Fine-grained prediction of food insecurity using news streams</u></b> <br>
		Ananth Balashankar, Lakshminarayanan Subramanian, Samuel Fraiberger <br> 
		<i>International Conference on Computational Social Science (IC2S2)</i> 2022 </li>
	<br>
	<li id="fair3"><b><u>The need for transparent demographic group trade-offs in Credit Risk and Income Classification</u></b> <br>
		Ananth Balashankar, Alyssa Lees<br>
		<i>iConference 2022.</i><a href="i_Conf_Fairness.pdf">[pdf]</a></li>
	<br>
	<li id="famine2"><b><u>Quantifying Risks of Food Insecurity by Analyzing News Media</u></b> <br>
		Ananth Balashankar, Lakshminarayanan Subramanian, Samuel Fraiberger <br> 
		<i>International Conference on Computational Social Science (IC2S2)</i> 2020.
		<i> Contributed Talk at INFORMS 2021, World Bank Conference on AI in economic development, 2018.</i> </li>
	<br>
	<li id="faithful"><b><u>Learning Faithful Representations of Causal Graphs</u></b> <br>
		Ananth Balashankar, Lakshminarayanan Subramanian<br>
		<i>Conference of Association of Computational Linguistics (ACL) 2021 (Oral Paper Acceptance Rate: <6%) </i><a href="https://aclanthology.org/2021.acl-long.69/">[pdf]</a></li>
	<br>
	<li id="concordance"><b><u>Enhancing Neural Recommender Models through Domain-Specific Concordance</u></b> <br>
		Ananth Balashankar, Alex Beutel, Lakshminarayanan Subramanian<br>
		<i>International Conference on Web Search and Data Mining (WSDM) 2021 (Acceptance Rate: 18.6%) </i> <a href="https://dl.acm.org/doi/10.1145/3437963.3441784">[pdf]</a></li>
	<br>
	<li id="pcg1"><b><u>Identifying Predictive Causal Factors from News Streams</u></b> <br>
		Ananth Balashankar, Sunandan Chakraborty, Samuel Fraiberger, Lakshminarayanan Subramanian<br>
		<i>Conference on Empirical Methods in Natural Language Processing (EMNLP) 2019 (Oral Paper Acceptance Rate: <7%)</i> <a href="https://drive.google.com/file/d/1ThqD6B1RoZPBKdLiZNQWSxgIC23afrJ6/view?usp=sharing">[pdf]</a></li>
	<br>
	<li id="health"><b><u>Reconstructing the MERS Disease Outbreak from News</u></b> <br>
		Ananth Balashankar, Aashish Dugar, Lakshmi Subramanian, Samuel Fraiberger<br>
		<i> ACM Computing and Sustainable Societies (COMPASS)</i> 2019. <a href="https://dl.acm.org/citation.cfm?id=3332498">[pdf] </a> </li>
	<br>
	<li><b><u>Predicting Angiographic Disease Status: Where to draw the line between demographically decoupled and jointly trained models?</u></b> <br>
		Ananth Balashankar, Alyssa Lees, Srikanth Jagabathula, Lakshminarayanan Subramanian.
		<i>Under Review.</i> <a href="pareto_efficiency.pdf">[pdf]</a></li>
	<br>
<!-- 	<li id="pollution2"><b><u>Localized Pollution Hotspots: Inferences from a Three-year Fine-grained  Air Quality Monitoring Study in Delhi</u></b> <br>
		Shiva Iyer, Ananth Balashankar, Rohini Pande, Anant Sudarshan, Lakshminarayanan Subramanian.<br> Under Preparation.</li>
	<br> -->
	<li id="fair1"><b><u>Pareto Efficient Fairness for Skewed Subgroup Data</u></b> <br>
		Ananth Balashankar, Alyssa Lees, Chris Welty, Lakshmi Subramanian <br>
		<i> ICML workshop on AI for Social Good</i> 2019.
		[<a href="https://aiforsocialgood.github.io/icml2019/accepted/track1/pdfs/24_aisg_icml2019.pdf">Best Paper!</a>]
		[<a href="https://arxiv.org/abs/1910.14120">arxiv</a>]</li>
	<br>
	<li><b><u>Fairness Sample Complexity and the Case for Human Intervention</u></b> <br>
		Ananth Balashankar, Alyssa Lees <br>
		<i> International Conference on Human Factors in Computing Systems (CHI) 2019 - Bridging the Gap Between AI and HCI Workshop.</i> <a href="https://ai.google/research/pubs/pub48036">[pdf]</a> </li>
	<br>
	<li><b><u>Unsupervised Word Influencer Networks from news streams</u></b> <br>
		Ananth Balashankar, Sunandan Chakraborty, Lakshmi Subramanian.  <br>
		<i> ACL Workshop on Economics and Natural Language Processing (ECONLP) 2018</i> <a href="http://www.aclweb.org/anthology/W18-3109">[pdf]</a> </li>
	<br>
	<li><b><u>Towards Applying Open Domain Question Answering to Privacy Policies</u></b> <br>
		Yan Shvartzshnaider, Ananth Balashankar, Thomas Wies, Lakshminarayanan Subramanian.  <br>
		<i> ACL Workshop on Machine Reading for Question Answering (MRQA) 2018</i> <a href="http://www.aclweb.org/anthology/W18-2608">[pdf]</a></li>
	<br>
	<li><b><u>Causal Inference from News Streams</u></b> <br>
		Ananth Balashankar, Sunandan Chakraborty, Samuel Fraiberger, Srikanth Jagabathula, Lakshminarayanan Subramanian.  <br>
		<i> ICML Workshop on Machine Learning for Causal Inference, Counterfactual Prediction, and Autonomous Action (CausalML) 2018</i> <a href="https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxmYWltMTh3c2NhdXNhbG1sfGd4OjUwNmI1ZTUwNzgzMTk5OWQ">[pdf]</a></li>
	<br>
	<li><b><u>Stable virtual landmarks: Spatial dropbox to enhance retail experience</u> </b><br>
		Swadhin Pradhan, Ananth Balashankar, Niloy Ganguly and Bivas Mitra<br>
		<i> Communication Systems and Networks (COMSNETS) </i> 2014  <a href="https://ieeexplore.ieee.org/abstract/document/6734891/">[pdf]</a> </li>
	<br>
	<li><b><u>Signal-aware data transfer in cellular networks</u></b> <br>
		Vishnu Navda, Ramachandran Ramjee, Sahil Suneja, Ananth Balashankar<br>
		<i> US Patent </i> <a href="https://patents.google.com/patent/US8843169">8843169</a> </li>
	<br>
	<li id="play"><b><u>App Discovery with Google Play: Personalized Recommendations with Related Apps</u></b> <br>
		Ananth Balashankar, Levent Koc, Norberto Guimaraes<br>
		<i> Google AI Blog</i> 2016. <a href="https://ai.googleblog.com/2016/12/app-discovery-with-google-play-part-2.html">[blog]</a> </li> <br>
	
	</ul>
	
	<h2> Teaching Experience </h2>
	<ul>
	<li><b>Big Data and ML Systems</b> (CSCI-GA. 3033-016, Spring 2019 - New York University)
	Designed and taught lab sessions for a class of MS in Computer Science and Computer Engineering, Entrepreneurship and Innovation (MS-CEI) students, 
	on Spark distributed ML computing platform, PageRank algorithm, deep learning neural network models for text processing,
	image recognition, graph learning, multi-arm bandits, recommender systems and healthcare inference. </li><br>
	
	<li><b>Foundations of Networks and Mobile Systems</b> (CSCI-GA. 2630-001 and 002, Fall 2021  - New York University)
	Designed and taught lab sessions for a class of 100+ students from Tech MBA, MS-CEI programs with hands-on lab sessions on internet technologies like DNS, HTML, JavaScript,SQL, 
	PHP, React, etc. This was an introductory course that exposed students to the fundamentals of computer networks and mobile systems.</li><br>
	
	<li><b>Operating Systems</b> (CS30002, Spring 2014, IIT Kharagpur)
	Designed and conducted lab sessions for 120+ undergraduate CS students for topics on file systems, schedulers, etc in Ubuntu OS</li><br>
	
	<li><b>Programming and Data Structures</b> (CS 11001, Fall 2013, IIT Kharagpur)
	Designed and conducted lab sessions for 150+ undergraduate CS students for introductory topics in C programming</li>
	</ul>
	
</div> <!--left container-->

<!--/div--> <!--wrapper -->

</body>

</html>
